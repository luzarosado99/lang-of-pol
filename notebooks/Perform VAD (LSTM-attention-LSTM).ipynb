{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2106d8d8",
   "metadata": {},
   "source": [
    "## Load Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c2e6aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7faef854f290>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio.transforms as T\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c14937",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9943a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# f = '/home/anishk/VAD/Source/Data/data1.pt'\n",
    "# data = torch.load(f)\n",
    "# data = np.transpose(data, (0, 2, 1))\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "file = open('/media/4tb/data/Zone1/2018_08_04/2018_08_04vad_dict.pkl','rb')\n",
    "vad_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b4e13",
   "metadata": {},
   "source": [
    "## Define Audio File Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09737d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class audio_file():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.vad_slices = None\n",
    "        self.frames = None\n",
    "        self.frames_labels = None\n",
    "        self.mfcc = None\n",
    "    \n",
    "    def get_slices(self, vad_dict):\n",
    "        self.vad_slices = vad_dict[self.name]['pydub'][-24]['nonsilent_slices']\n",
    "        return self.vad_slices\n",
    "    \n",
    "    def get_frames(self):\n",
    "        \n",
    "        ms_2_sample = self.sample_rate/1000\n",
    "        frames_array = np.zeros(self.mfcc.shape[2])\n",
    "        #frames_array = np.zeros(180409)\n",
    "\n",
    "        for v in self.vad_slices:\n",
    "            start = math.floor(v[0]*ms_2_sample)\n",
    "            end = math.ceil(v[1]*ms_2_sample)\n",
    "\n",
    "            for i in range(start,end):\n",
    "                n = math.floor(i/220)\n",
    "                j = i%220\n",
    "                if j <= 110:\n",
    "                    frames_array[n-2] += 1\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=111 and j<=220:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=221 and j<=330:\n",
    "                    frames_array[n-1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                elif j>=331 and j<=440:\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "                elif j>=441:\n",
    "                    frames_array[n+2] += 1\n",
    "                    frames_array[n+1] += 1\n",
    "                    frames_array[n] += 1\n",
    "            \n",
    "            self.frames = frames_array\n",
    "            return self.frames\n",
    "        \n",
    "    def get_labels(self): \n",
    "        self.frames_labels = np.zeros(len(self.frames))\n",
    "        self.frames_labels[np.where(self.frames>0)] = 1\n",
    "        return self.frames_labels\n",
    "    \n",
    "    def get_mfcc(self): \n",
    "        file_name = '/media/4tb/data/Zone1/2018_08_04/' + self.name\n",
    "        self.waveform, self.sample_rate = torchaudio.load(file_name)\n",
    "        n_fft = 2048\n",
    "        win_length = 551\n",
    "        hop_length = 220\n",
    "        n_mels = 40\n",
    "        n_mfcc = 40\n",
    "\n",
    "        mfcc_transform = T.MFCC(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_mfcc=n_mfcc,\n",
    "            melkwargs={\n",
    "              'n_fft': n_fft,\n",
    "              'n_mels': n_mels,\n",
    "              'hop_length': hop_length,\n",
    "              'mel_scale': 'htk',\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.mfcc = mfcc_transform(self.waveform)\n",
    "        return self.mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51def098",
   "metadata": {},
   "source": [
    "## Instantiate Audio File Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812145ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obj_list = []\n",
    "input = []\n",
    "labels = []\n",
    "\n",
    "for key in vad_dict:\n",
    "    a = audio_file(key)\n",
    "    a.get_slices(vad_dict)\n",
    "    input.append(a.get_mfcc()[:,:,:180409])\n",
    "    a.get_frames()\n",
    "    labels.append(a.get_labels()[:180409])\n",
    "    #obj_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e13a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = input[:31:]\n",
    "input = torch.cat(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b8eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape\n",
    "input = torch.transpose(input,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de09236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.transpose(input,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64d96292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([180409, 31, 40])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fa5b80",
   "metadata": {},
   "source": [
    "## Reshape and Convert as Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b13a3344",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels[:31:]\n",
    "labels = np.stack(labels)\n",
    "labels = torch.from_numpy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e632b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = input[:31:]\n",
    "input = torch.cat(input)\n",
    "# input = torch.transpose(input,1,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83491be3",
   "metadata": {},
   "source": [
    "## Define Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c12cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.input_dim1 = 40\n",
    "        self.input_dim2 = 64\n",
    "        self.hidden_dim = 64\n",
    "        self.n_layers = 3\n",
    "        self.batch_size = 31\n",
    "        self.hidden_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.cell_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.hidden_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.cell_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.lstm1 = nn.LSTM(input_size = self.input_dim1, hidden_size = self.hidden_dim, num_layers = self.n_layers, batch_first=False)\n",
    "        self.lstm2 = nn.LSTM(input_size = self.input_dim2, hidden_size = self.hidden_dim, num_layers = self.n_layers, batch_first=False)\n",
    "        self.lstm2_out = None \n",
    "        self.hidden = None\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.convolve1d = nn.Sequential(\n",
    "            nn.Conv1d(3,3, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(3,5, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(5,5, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64, affine=False, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(5,1, kernel_size=11, padding=5)\n",
    "        )\n",
    "        self.output_stack = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "#     def create_rand_hidden1(self):\n",
    "#         self.hidden_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         self.cell_state1 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         return (self.hidden_state1, self.cell_state1)\n",
    "\n",
    "    def temp_attention(self, data):\n",
    "        self.output, hidden = self.lstm1(data, (self.hidden_state1, self.cell_state1))\n",
    "        self.H = self.output[0]\n",
    "        self.H_maxtemp = torch.max(self.H, 1).values\n",
    "        self.H_avgtemp = torch.mean(self.H, 1)\n",
    "        self.H_stdtemp = torch.std(self.H, 1)\n",
    "        self.H_concattemp = torch.cat([self.H_maxtemp[None, :], self.H_avgtemp[None, :], self.H_stdtemp[None,:]], dim=0)\n",
    "        return self.H_concattemp[None,:]   \n",
    "    \n",
    "    def convolve1(self, data):\n",
    "        self.H_temp = self.convolve1d(self.temp_attention(data))\n",
    "        # \"Expand/copy\" output of last layer (H_temp) to same dims as H\n",
    "        self.H_temp = self.H_temp.expand(-1,64,-1)\n",
    "        # Sigmoid activation     \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        self.input = self.H_temp\n",
    "        self.H_temp = sigmoid(self.input)\n",
    "        self.H_temp = torch.transpose(self.H_temp, 1, 2)[0]\n",
    "        # Merge H_temp and H by element wise summation\n",
    "        self.H_prime = torch.stack((self.H,self.H_temp))\n",
    "        self.H_prime = torch.sum(self.H_prime,0)\n",
    "        return self.H_prime[None,:]\n",
    "        \n",
    "#     def create_rand_hidden2(self):\n",
    "#         self.hidden_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         self.cell_state2 = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "#         return (self.hidden_state2, self.cell_state2)  \n",
    "    \n",
    "#     def freq_attention(hidden_feature_map):\n",
    "#         H_maxfreq = torch.max(hidden_feature_map, 0).values\n",
    "#         H_avgfreq = torch.mean(hidden_feature_map, 0)\n",
    "#         H_stdfreq = torch.std(hidden_feature_map, 0)\n",
    "#         H_concatfreq = torch.cat([H_maxfreq[None, :], H_avgfreq[None, :], H_stdfreq[None,:]], dim=0)\n",
    "#         return H_concatfreq \n",
    "\n",
    "    def forward(self, data):\n",
    "        self.input = self.convolve1(data)\n",
    "        self.lstm2_out, self.hidden = self.lstm2(self.input, (self.hidden_state2, self.cell_state2))\n",
    "        self.output = self.output_stack(self.lstm2_out[0])\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7929dab6",
   "metadata": {},
   "source": [
    "## Define Focal Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abfac4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=1,reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0789a75e",
   "metadata": {},
   "source": [
    "## Instantiate Model and Focal Loss Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ccee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackedLSTM()\n",
    "loss_fn = FocalLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd350c4e",
   "metadata": {},
   "source": [
    "## Calculate Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7fad468a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 64])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden_state1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "645cebe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 64])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden_state2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1946648d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-985336978b13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-79b8094d6746>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mce_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mce_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mfocal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2824\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer.zero_grad()\n",
    "loss_fn(model(input), labels).backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47a720a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 180409])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32d514",
   "metadata": {},
   "source": [
    "## Initialize LSTM\n",
    "Pytorchâ€™s LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The first axis is the sequence itself, the second indexes instances in the mini-batch, and the third indexes elements of the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "079791fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 40\n",
    "hidden_dim = 64 \n",
    "n_layers = 3 \n",
    "\n",
    "lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)\n",
    "\n",
    "out, hidden = lstm(data, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7f9dc327",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(40, 64, 3, batch_first=True)\n",
    "out, hidden = lstm(data, hidden)\n",
    "# nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b2a8ca33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c060b",
   "metadata": {},
   "source": [
    "## Apply attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00a9e5b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def temp_attention(hidden_feature_map):\n",
    "    H_maxtemp = torch.max(hidden_feature_map, 1).values\n",
    "    H_avgtemp = torch.mean(hidden_feature_map, 1)\n",
    "    H_stdtemp = torch.std(hidden_feature_map, 1)\n",
    "    H_concattemp = torch.cat([H_maxtemp[None, :], H_avgtemp[None, :], H_stdtemp[None,:]], dim=0)\n",
    "    return H_concattemp\n",
    "\n",
    "def freq_attention(hidden_feature_map):\n",
    "    H_maxfreq = torch.max(hidden_feature_map, 0).values\n",
    "    H_avgfreq = torch.mean(hidden_feature_map, 0)\n",
    "    H_stdfreq = torch.std(hidden_feature_map, 0)\n",
    "    H_concatfreq = torch.cat([H_maxfreq[None, :], H_avgfreq[None, :], H_stdfreq[None,:]], dim=0)\n",
    "    return H_concatfreq \n",
    "\n",
    "def convolve(input,H):\n",
    "    # Define normalization and relu functions for use after first 3 convolutions\n",
    "    norm = nn.BatchNorm1d(64, affine=False, track_running_stats=False)\n",
    "    ReLU = nn.ReLU()\n",
    "\n",
    "    # 1D Convolution; padding of 5 on both sides to account for ndims change\n",
    "    conv1 = nn.Conv1d(3,3, kernel_size=11, padding=5)\n",
    "    output = conv1(input)\n",
    "    output = norm(output)\n",
    "    output = ReLU(output)\n",
    "    \n",
    "    conv2 = nn.Conv1d(3,5, kernel_size=11, padding=5)\n",
    "    input = output\n",
    "    output = conv2(input)\n",
    "    output = norm(output)\n",
    "    output = ReLU(output)\n",
    "    \n",
    "    conv3 = nn.Conv1d(5,5, kernel_size=11, padding=5)\n",
    "    input = output\n",
    "    output = conv3(input)\n",
    "    output = norm(output)\n",
    "    output = ReLU(output)\n",
    "    \n",
    "    conv4 = nn.Conv1d(5,1, kernel_size=11, padding=5)\n",
    "    input = output\n",
    "    H_temp = conv4(input)\n",
    "    # \"Expand/copy\" output of last layer (H_temp) to same dims as H\n",
    "    H_temp = H_temp.expand(-1,64,-1)\n",
    "    # Sigmoid activation     \n",
    "    sigmoid = nn.Sigmoid()\n",
    "    input = H_temp\n",
    "    H_temp = sigmoid(input)\n",
    "    H_temp = torch.transpose(H_temp, 1, 2)[0]\n",
    "    # Merge H_temp and H by element wise summation\n",
    "    H_prime = torch.stack((H,H_temp))\n",
    "    H_prime = torch.sum(H_prime,0)\n",
    "    return H_prime\n",
    "\n",
    "H = out[0] ##H is the \"Hidden feature map\"\n",
    "input = temp_attention(H)[None,:] ## batch_size, channels, features\n",
    "output = convolve(input,H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884bfc43",
   "metadata": {},
   "source": [
    "## Function for checking output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ef48e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def display(tensor):\n",
    "    input = tensor.detach().numpy()\n",
    "    a = plt.hist(input[0][0], bins = 50)\n",
    "    b = plt.hist(input[0][1], bins = 50)\n",
    "    c = plt.hist(input[0][2], bins = 50)\n",
    "    plt.show()\n",
    "    d = print('max:',np.max(input))\n",
    "    e = print('min:',np.min(input))\n",
    "    range = np.max(input)-np.min(input)\n",
    "    f = print('range:',range)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0100cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = out[0] ##H is the \"Hidden feature map\"\n",
    "input = temp_attention(H)[None,:] ## batch_size, channels, features\n",
    "\n",
    "# Define normalization and relu functions for use after first 3 convolutions\n",
    "norm = nn.BatchNorm1d(64, affine=False, track_running_stats=False)\n",
    "ReLU = nn.ReLU()\n",
    "\n",
    "# 1D Convolution; padding of 5 on both sides to account for ndims change\n",
    "conv1 = nn.Conv1d(3,3, kernel_size=11, padding=5)\n",
    "output = conv1(input)\n",
    "output = norm(output)\n",
    "output = ReLU(output)\n",
    "\n",
    "conv2 = nn.Conv1d(3,5, kernel_size=11, padding=5)\n",
    "input = output\n",
    "output = conv2(input)\n",
    "output = norm(output)\n",
    "output = ReLU(output)\n",
    "\n",
    "\n",
    "conv3 = nn.Conv1d(5,5, kernel_size=11, padding=5)\n",
    "input = output\n",
    "output = conv3(input)\n",
    "output = norm(output)\n",
    "output = ReLU(output)\n",
    "\n",
    "conv4 = nn.Conv1d(5,1, kernel_size=11, padding=5)\n",
    "input = output\n",
    "H_temp = conv4(input)\n",
    "# \"Expand/copy\" output of last layer (H_temp) to same dims as H\n",
    "H_temp = H_temp.expand(-1,64,-1)\n",
    "# Sigmoid activation     \n",
    "sigmoid = nn.Sigmoid()\n",
    "input = H_temp\n",
    "H_temp = sigmoid(input)\n",
    "H_temp = torch.transpose(H_temp, 1, 2)[0]\n",
    "# Merge H_temp and H by element wise summation\n",
    "H_prime = torch.stack((H,H_temp))\n",
    "H_prime = torch.sum(H_prime,0)\n",
    "display(H_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7408bd31",
   "metadata": {},
   "source": [
    "## Use outputs and hidden state from above as inputs to second LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9548b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same parameters as LSTM1 except for input dim\n",
    "\n",
    "output = output[None,:,:]\n",
    "input_dim = 64\n",
    "hidden_dim = 64 \n",
    "n_layers = 3 \n",
    "\n",
    "lstm2 = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)\n",
    "\n",
    "out, hidden = lstm2(output, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ed263",
   "metadata": {},
   "source": [
    "## Linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a5e44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = nn.Linear(64, 64)\n",
    "linear1_output = linear1(out)\n",
    "\n",
    "linear2 = nn.Linear(64, 1)\n",
    "linear2_output = linear2(linear1_output)\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "input = linear2_output\n",
    "output_binary = sigmoid(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e8f3d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 183714, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_binary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954bf6d7",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer.zero_grad()\n",
    "loss_fn(model(input), target).backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f260e1c",
   "metadata": {},
   "source": [
    "## Focal loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c048bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedFocalLoss(nn.Module):\n",
    "    \"Non weighted version of Focal Loss\"\n",
    "    def __init__(self, alpha=.25, gamma=1):\n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor([alpha, 1-alpha]).cuda()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        targets = targets.type(torch.long)\n",
    "        at = self.alpha.gather(0, targets.data.view(-1))\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = at*(1-pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43fb46a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
